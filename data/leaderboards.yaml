# Website Title (displayed in purple header at the top)
website_title: Live-SWE-agent

# Website Subtitle (optional, displayed below the title with smaller, lighter text)
website_subtitle: Can Software Engineering Agents to Self-Evolve on the Fly?

# Section Title (the word shown above the tabs, default is "Leaderboard")
section_title: Leaderboard

# Section Description (displayed below the section title, supports HTML)
section_description: 'All results are evaluated using our <a href="https://github.com/OpenAutoCoder/live-swe-agent" target="_blank" rel="noopener noreferrer" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; font-weight: bold; padding: 0.125rem 0.375rem; border-radius: 0.25rem; text-decoration: none;">Live-SWE-agent</a> scaffold. The <span class="badge badge-verified" style="padding: 0.0625rem 0.25rem; font-size: 0.7rem;">âœ“</span> badge indicates that results have been verified. <br> Submit your model''s score with our scaffold for an apples-to-apples comparison!' 

# Footer Links Configuration
footer_links:
  - name: GitHub
    url: https://github.com/OpenAutoCoder/live-swe-agent
  # - name: HuggingFace
  #   url: https://huggingface.co/ise-uiuc
  - name: Paper
    url: https://arxiv.org/abs/2511.13646

# ==============================================================================
# Leaderboards Configuration
# ==============================================================================
# Each leaderboard represents a separate benchmark/dataset
# You can add as many leaderboards as you want, and they will appear as tabs
#
# Fields:
#   display_name: The text shown in the tab (required, can be anything)
#   info_sections: List of information blocks shown at bottom (optional)
#     - title: Section title
#       content: Section content
#   results: List of model results
#
# Note: 'name' is auto-generated from display_name (lowercase + underscores)
# ==============================================================================

leaderboards:
  - display_name: SWE-bench Verified
    info_sections:
      - title: 'ðŸ“£ News'
        content: |
          <ul>
            <li><span style="font-weight: bold;">[Nov 24th, 2025]:</span> <strong>Claude Opus 4.5 + Live-SWE-agent</strong> scores <strong>79.2%</strong> on SWE-bench Verified, leading all current open-source scaffolds and coming very close to Anthropicâ€™s internal, manually engineered scaffold for Opus 4.5!!</li>
            <li><span style="font-weight: bold;">[Nov 20th, 2025]:</span> <strong>Gemini 3 Pro + Live-SWE-agent</strong> scores <strong>77.4%</strong> on SWE-bench Verified, outperforming all available models (including Claude Sonnet 4.5) at the time of writing!</li>
            <li><span style="font-weight: bold;">[Nov 17th, 2025]:</span> <strong>Claude Sonnet 4.5 + Live-SWE-agent</strong> achieves the new state-of-the-art solve rate of <strong>45.8%</strong> on SWE-Bench Pro!</li>
            <li><span style="font-weight: bold;">[Nov 17th, 2025]:</span> We've released Live-SWE-agent 1.0.0!</li>
          </ul>
      - title: '<a href="https://arxiv.org/abs/2511.13646" target="_blank" rel="noopener noreferrer">Live-SWE-agent</a>'
        content: |
          Live-SWE-agent is the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems.
          <br>
          Figure below presents an overview of Live-SWE-agent.
          <br>
          <img src="img/overview.png" alt="Live-SWE-agent Overview" style="max-width: 80%; height: auto; display: block; margin: 0 auto;">
          <br>
          First, <span style="display: inline-block; width: 1.2em; height: 1.2em; line-height: 1.2em; text-align: center; border-radius: 50%; background-color: #a64d79; color: white; font-size: 0.8em;">1</span> the agent take in both the project codebase and the description of the issue to be solved with only a limited set of tools (e.g., bash commands), aiming to generate and use its own tools on the fly while solving the issue.
          <br>
          During execution, at each step, it can choose to either <span style="display: inline-block; width: 1.2em; height: 1.2em; line-height: 1.2em; text-align: center; border-radius: 50%; background-color: #a64d79; color: white; font-size: 0.8em;">2</span> output a command (e.g., to use a tool) or <span style="display: inline-block; width: 1.2em; height: 1.2em; line-height: 1.2em; text-align: center; border-radius: 50%; background-color: #a64d79; color: white; font-size: 0.8em;">3</span> create a custom tool that can help it solve the issue. In Live-SWE-Agent, we define a custom tool as a script that can be executed in the environment.
          <br>
          Next, based on the <span style="display: inline-block; width: 1.2em; height: 1.2em; line-height: 1.2em; text-align: center; border-radius: 50%; background-color: #a64d79; color: white; font-size: 0.8em;">4</span> environmental feedback message, <span style="display: inline-block; width: 1.2em; height: 1.2em; line-height: 1.2em; text-align: center; border-radius: 50%; background-color: #a64d79; color: white; font-size: 0.8em;">5</span> we specifically ask the agent to reflect upon the past steps and decide whether a tool should be created.
          <br>
          This loop is repeated until the agent has submitted a solution <span style="display: inline-block; width: 1.2em; height: 1.2em; line-height: 1.2em; text-align: center; border-radius: 50%; background-color: #a64d79; color: white; font-size: 0.8em;">6</span> to the initial problem.
      - title: '<a href="https://openai.com/index/introducing-swe-bench-verified/" target="_blank" rel="noopener noreferrer">SWE-bench Verified</a>'
        content: SWE-bench Verified benchmark contains 500 software development problems where the goal is to successfully modify the repository given a problem description. SWE-bench Verified is validated by human developers to ensure each problem description has sufficient amount of information to solve the issue.

    results:
      # Proprietary Models
      - model: Claude Opus 4.5 (20251101)
        org: Anthropic
        # footnote: "We use temperature=1.0 for this model."
        resolved: 79.2
        avg_cost: 0.86
        date: "2025-11-24"
        open_source: false
        verified: true
        newest: true
        tags:
          - "Org: Anthropic"
          - "Type: Proprietary"

      - model: Gemini 3 Pro Preview (20251118)
        org: Google
        footnote: "We use temperature=1.0 for this model."
        resolved: 77.4
        avg_cost: 0.48
        date: "2025-11-20"
        open_source: false
        verified: true
        newest: true
        tags:
          - "Org: Google"
          - "Type: Proprietary"

      - model: Claude Sonnet 4.5 (20250929)
        org: Anthropic
        resolved: 75.4
        avg_cost: 0.68
        date: "2025-11-17"
        open_source: false
        verified: true
        tags:
          - "Org: Anthropic"
          - "Type: Proprietary"

      - model: GPT-5 (20250807)
        org: OpenAI
        resolved: 68.4
        avg_cost: 0.27
        date: "2025-11-17"
        open_source: false
        verified: true
        tags:
          - "Org: OpenAI"
          - "Type: Proprietary"

      - model: GPT-5-mini (20250807)
        org: OpenAI
        resolved: 63.0
        avg_cost: 0.05
        date: "2025-11-17"
        open_source: false
        verified: true
        tags:
          - "Org: OpenAI"
          - "Type: Proprietary"


  - display_name: SWE-Bench Pro
    info_sections:
      - title: '<a href="https://scale.com/leaderboard" target="_blank" rel="noopener noreferrer">SWE-Bench Pro</a>'
        content: SWE-Bench Pro contains 731 publicly-available problems, aimed to capture realistic, complex, enterprise-level problems. Compared with SWE-bench Verified, SWE-Bench Pro contains more difficult problems across multiple repositories and programming languages.
    results:
     # Proprietary Models
      - model: Claude 4.5 Sonnet (20250929)
        org: Anthropic
        resolved: 45.8
        avg_cost: 0.73
        date: "2025-11-15"
        open_source: false
        verified: true
        tags:
          - "Org: Anthropic"
          - "Type: Proprietary"
